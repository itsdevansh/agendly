{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "PDF_FOLDER = \"/Users/devanshk/Desktop/Deepseek-r1-local-chatbot/rag-data\"\n",
    "CHROMA_DB_PATH = \"./chroma_db_persistent\"  # Directory to store persistent ChromaDB data\n",
    "COLLECTION_NAME = \"pdf_semantic_chunks\"\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # Make sure this model is compatible with SentenceTransformerEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /Users/devanshk/miniforge3/lib/python3.12/site-packages (4.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/devanshk/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client at: ./chroma_db_persistent\n",
      "Using embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "# --- Initialize ChromaDB Client (Persistent) ---\n",
    "print(f\"Initializing ChromaDB client at: {CHROMA_DB_PATH}\")\n",
    "# Creates the directory if it doesn't exist and persists data there\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "# --- Initialize Embedding Function ---\n",
    "print(f\"Using embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "# Using a helper function from ChromaDB Utils for Sentence Transformers\n",
    "# This handles downloading the model and generating embeddings automatically within ChromaDB\n",
    "chroma_embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting or creating ChromaDB collection: pdf_semantic_chunks\n"
     ]
    }
   ],
   "source": [
    "# --- Get or Create ChromaDB Collection ---\n",
    "print(f\"Getting or creating ChromaDB collection: {COLLECTION_NAME}\")\n",
    "# Pass the embedding function during collection creation/retrieval\n",
    "# ChromaDB will use this function automatically when you add documents if embeddings aren't provided\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=chroma_embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Optional: Specifies the distance metric (cosine is common for text embeddings)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Text Splitter (using LangChain for structure-aware chunking) ---\n",
    "# This splitter tries to keep paragraphs/sentences together, which is a form of semantic chunking.\n",
    "# Adjust chunk_size and chunk_overlap as needed for your specific documents and use case.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Max characters per chunk\n",
    "    chunk_overlap=150,       # Characters to overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"], # Order of separators to try\n",
    "    length_function=len,     # Function to measure chunk size (usually len)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting PDF processing from folder: /Users/devanshk/Desktop/Deepseek-r1-local-chatbot/rag-data\n",
      "Found 3 PDF files to process.\n",
      "\n",
      "Processing: /Users/devanshk/Desktop/Deepseek-r1-local-chatbot/rag-data/Data Document.pdf...\n",
      " - Extracted text and split into 90 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 0 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Added 90 chunks to ChromaDB collection 'pdf_semantic_chunks'.\n",
      "\n",
      "Processing: /Users/devanshk/Desktop/Deepseek-r1-local-chatbot/rag-data/The Now Habit.pdf...\n",
      " - Extracted text and split into 19 chunks.\n",
      " - Added 19 chunks to ChromaDB collection 'pdf_semantic_chunks'.\n",
      "\n",
      "Processing: /Users/devanshk/Desktop/Deepseek-r1-local-chatbot/rag-data/Deep Work by Cal Newport.pdf...\n",
      " - Extracted text and split into 21 chunks.\n",
      " - Added 21 chunks to ChromaDB collection 'pdf_semantic_chunks'.\n",
      "\n",
      "--- Processing Complete ---\n",
      "Successfully processed 3 out of 3 PDF files.\n",
      "Total chunks added to ChromaDB: 130\n",
      "Total time taken: 2.94 seconds\n",
      "ChromaDB collection 'pdf_semantic_chunks' now contains 130 items.\n",
      "Persistent data stored in: /Users/devanshk/Downloads/Deepseek-r1-local-chatbot/chroma_db_persistent\n"
     ]
    }
   ],
   "source": [
    "# --- Process PDF Files ---\n",
    "print(f\"\\nStarting PDF processing from folder: {PDF_FOLDER}\")\n",
    "processed_files = 0\n",
    "total_chunks_added = 0\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.isdir(PDF_FOLDER):\n",
    "    print(f\"Error: Folder '{PDF_FOLDER}' not found.\")\n",
    "    exit()\n",
    "\n",
    "pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files to process.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "    print(f\"\\nProcessing: {pdf_path}...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Read PDF\n",
    "        reader = PdfReader(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                full_text += f\"\\n--- Page {i+1} ---\\n\" + page_text # Add page breaks for context\n",
    "            else:\n",
    "                 print(f\" - Warning: Could not extract text from page {i+1} of {pdf_file}\")\n",
    "\n",
    "\n",
    "        if not full_text.strip():\n",
    "            print(f\" - Warning: No text extracted from {pdf_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Perform Semantic Chunking (Structure-Aware Splitting)\n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "        if not chunks:\n",
    "            print(f\" - Warning: Text splitting resulted in no chunks for {pdf_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\" - Extracted text and split into {len(chunks)} chunks.\")\n",
    "\n",
    "        # 3. Prepare Data for ChromaDB\n",
    "        ids = [f\"{pdf_file}_chunk_{i}_{uuid.uuid4()}\" for i in range(len(chunks))] # Unique IDs are crucial\n",
    "        documents = chunks # The actual text content of each chunk\n",
    "        metadatas = [{\"source_file\": pdf_file, \"original_chunk_index\": i} for i in range(len(chunks))] # Metadata per chunk\n",
    "\n",
    "        # 4. Add to ChromaDB Collection\n",
    "        # ChromaDB will automatically use the collection's embedding function\n",
    "        # to generate embeddings for the 'documents' being added.\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\" - Added {len(chunks)} chunks to ChromaDB collection '{COLLECTION_NAME}'.\")\n",
    "        processed_files += 1\n",
    "        total_chunks_added += len(chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - !!! Error processing {pdf_file}: {e}\")\n",
    "        # Optionally add more robust error handling (e.g., logging to a file)\n",
    "\n",
    "# --- Final Summary ---\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")\n",
    "print(f\"Successfully processed {processed_files} out of {len(pdf_files)} PDF files.\")\n",
    "print(f\"Total chunks added to ChromaDB: {total_chunks_added}\")\n",
    "print(f\"Total time taken: {elapsed_time:.2f} seconds\")\n",
    "try:\n",
    "    print(f\"ChromaDB collection '{COLLECTION_NAME}' now contains {collection.count()} items.\")\n",
    "    print(f\"Persistent data stored in: {os.path.abspath(CHROMA_DB_PATH)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get final count from ChromaDB collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
